{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"restricted_boltzmann_machine.ipynb","provenance":[{"file_id":"1JxIEe_TAcz-utfLKTqjbo3foOtqDGD0s","timestamp":1629990943830}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K4f4JG1gdKqj"},"source":["#Boltzmann Machine"]},{"cell_type":"markdown","metadata":{"id":"1jbiqOK7dLGG"},"source":["##Downloading the dataset"]},{"cell_type":"markdown","metadata":{"id":"XL5MEkLcfRD2"},"source":["###ML-100K"]},{"cell_type":"code","metadata":{"id":"rjOPzue7FCXJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630161050645,"user_tz":-300,"elapsed":1194,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"3ff568b8-8187-4bf9-fbcc-da79cb8a564a"},"source":["!wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n","!unzip ml-100k.zip\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2021-08-28 14:30:48--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n","Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n","Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4924029 (4.7M) [application/zip]\n","Saving to: ‘ml-100k.zip’\n","\n","ml-100k.zip         100%[===================>]   4.70M  24.4MB/s    in 0.2s    \n","\n","2021-08-28 14:30:49 (24.4 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n","\n","Archive:  ml-100k.zip\n","   creating: ml-100k/\n","  inflating: ml-100k/allbut.pl       \n","  inflating: ml-100k/mku.sh          \n","  inflating: ml-100k/README          \n","  inflating: ml-100k/u.data          \n","  inflating: ml-100k/u.genre         \n","  inflating: ml-100k/u.info          \n","  inflating: ml-100k/u.item          \n","  inflating: ml-100k/u.occupation    \n","  inflating: ml-100k/u.user          \n","  inflating: ml-100k/u1.base         \n","  inflating: ml-100k/u1.test         \n","  inflating: ml-100k/u2.base         \n","  inflating: ml-100k/u2.test         \n","  inflating: ml-100k/u3.base         \n","  inflating: ml-100k/u3.test         \n","  inflating: ml-100k/u4.base         \n","  inflating: ml-100k/u4.test         \n","  inflating: ml-100k/u5.base         \n","  inflating: ml-100k/u5.test         \n","  inflating: ml-100k/ua.base         \n","  inflating: ml-100k/ua.test         \n","  inflating: ml-100k/ub.base         \n","  inflating: ml-100k/ub.test         \n","ml-100k  ml-100k.zip  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9Xis6ldDfTs6"},"source":["###ML-1M"]},{"cell_type":"code","metadata":{"id":"LOly1yfAfTjd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630161051559,"user_tz":-300,"elapsed":923,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"6d5e22c8-f548-441b-87b7-12693c114cbd"},"source":["!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n","!unzip ml-1m.zip\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-08-28 14:30:49--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\n","Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n","Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5917549 (5.6M) [application/zip]\n","Saving to: ‘ml-1m.zip’\n","\n","ml-1m.zip           100%[===================>]   5.64M  32.7MB/s    in 0.2s    \n","\n","2021-08-28 14:30:50 (32.7 MB/s) - ‘ml-1m.zip’ saved [5917549/5917549]\n","\n","Archive:  ml-1m.zip\n","   creating: ml-1m/\n","  inflating: ml-1m/movies.dat        \n","  inflating: ml-1m/ratings.dat       \n","  inflating: ml-1m/README            \n","  inflating: ml-1m/users.dat         \n","ml-100k  ml-100k.zip  ml-1m  ml-1m.zip\tsample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EOBJ8UCXdY0g"},"source":["##Importing the libraries"]},{"cell_type":"code","metadata":{"id":"_LvGeU1CeCtg","executionInfo":{"status":"ok","timestamp":1630161056310,"user_tz":-300,"elapsed":4757,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.autograd import Variable"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pM04FyMudkoK"},"source":["## Importing the dataset\n"]},{"cell_type":"code","metadata":{"id":"UJw2p3-Cewo4","executionInfo":{"status":"ok","timestamp":1630161067855,"user_tz":-300,"elapsed":11550,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["# We won't be using this dataset.\n","# import the dataset from ml-1m (large dataset)\n","movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n","#columns: id_movies, movie_names, movie_genre\n","users = pd.read_csv(\"ml-1m/users.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n","#users column: id_user, gender, age, job code\n","ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", header=None, engine=\"python\", encoding=\"latin-1\")\n","#ratings column: id_user, id_movie, ratings, timestamps"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTIbE2tkdkwP"},"source":["## Preparing the training set and the test set\n"]},{"cell_type":"code","metadata":{"id":"2usLKJBEgPE2","executionInfo":{"status":"ok","timestamp":1630161067860,"user_tz":-300,"elapsed":12,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["training_set = pd.read_csv(\"ml-100k/u1.base\", delimiter=\"\\t\",) #contains user, id_movie, rating, and timestamp\n","training_set = np.array(training_set, dtype=\"int\") #torch accepts numpy arrays\n","test_set = pd.read_csv(\"ml-100k/u1.base\", delimiter=\"\\t\",) #contains user, id_movie, rating, and timestamp\n","test_set = np.array(test_set, dtype=\"int\") #torch accepts numpy arrays"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCf8HjSydk4s"},"source":["## Getting the number of users and movies\n"]},{"cell_type":"code","metadata":{"id":"gPaGZqdniC5m","executionInfo":{"status":"ok","timestamp":1630161067861,"user_tz":-300,"elapsed":11,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["#maximum number of users and movies\n","nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n","nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-w4-hVidlAm"},"source":["## Converting the data into an array with users in lines and movies in columns\n"]},{"cell_type":"code","metadata":{"id":"-wASs2YFiDaa","executionInfo":{"status":"ok","timestamp":1630161068282,"user_tz":-300,"elapsed":431,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["#users in rows, movies in columns and ratings in cells\n","# needed for pytorch, create a list of lists\n","def convert(data):\n","    new_data=[]\n","    for id_users in range(1, nb_users+1):\n","        id_movies = data[:, 1] [data[: ,0] == id_users]\n","        id_ratings = data[:,2][data[:,0]==id_users]\n","        ratings = np.zeros(nb_movies)\n","        ratings[id_movies-1]=id_ratings #movies not rated are filled with zero\n","        new_data.append(list(ratings))\n","    return new_data\n","\n","training_set = convert(training_set)\n","test_set = convert(test_set)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMmhuUpldlHo"},"source":["## Converting the data into Torch tensors\n"]},{"cell_type":"code","metadata":{"id":"TwD-KD8yiEEw","executionInfo":{"status":"ok","timestamp":1630161068595,"user_tz":-300,"elapsed":315,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["#needed for ML and DL, better to build tensor arrays (more efficient that numpy arrays)\n","#FloatTensor expects a list of lists\n","training_set = torch.FloatTensor(training_set)\n","test_set = torch.FloatTensor(test_set)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIPruubGdlPW"},"source":["## Converting the ratings into binary ratings 1 (Liked) or 0 (Not Liked)\n"]},{"cell_type":"code","metadata":{"id":"cslbPSh6iEka","executionInfo":{"status":"ok","timestamp":1630161069181,"user_tz":-300,"elapsed":589,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["#replace all zero ratings to -1 (no rating)\n","training_set[training_set ==0]= -1\n","training_set[training_set ==1]= 0\n","training_set[training_set ==2]= 0\n","training_set[training_set >=0]= 1\n","\n","test_set[test_set ==0]= -1\n","test_set[test_set ==1]= 0\n","test_set[test_set ==2]= 0\n","test_set[test_set >=0]= 1"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kkL8NkkdlZj"},"source":["## Creating the architecture of the Neural Network\n"]},{"cell_type":"code","metadata":{"id":"oU2nyh76iE6M","executionInfo":{"status":"ok","timestamp":1630161069182,"user_tz":-300,"elapsed":7,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["#probabilistic graphical model (use classes)\n","#class should define hidden nodes, weights of prob of visible nodes given the hidden nodes,\n","#bias, bias of visible nodes given the hidden nodes(Created 4 functions)\n","class RBM():\n","    def __init__(self, nv, nh):\n","        self.W = torch.randn(nh, nv) #prob of visible nodes given the hidden nodes\n","        self.a = torch.randn(1, nh) #bias of prob of hidden nodes given the visible nodes\n","        self.b = torch.randn(1, nv) #bias of prob of visible nodes given the hidden nodes\n","#created an additional fake dimension to the biases using 1 because pytorch cant accept a single \n","#input vector of 1D as argument(1=  fake batch)\n","\n","#sampling the hidden nodes according to the prob p(h) given v. This is the same as the \n","#Sigmoid Activation Function******\n","#we need the sample h function  because during the trainingm we'll need to approximate\n","#the log likelihood gradient using Gibb's Sampling\n","    def sample_h(self, x):  \n","#x = given visible neurons\n","#prob of h given v is the Sigmoid Activation Function applied to wx+a (a = bias of hidden node)\n","        wx = torch.mm(x, self.W.t())    \n","#transpose cos hidden nodes, mm for pytorch tensor product, activation=wx+a(hidden)\n","        activation = wx + self.a.expand_as(wx)  \n","#expand cos each input vector will not be treated individually but inside batches\n","#we have mini batches from self.a. To make sure that the bias(self.a) is applied to each line\n","#of the mini batch[our dimensions in self.a=torch.randn(1,nh)], we add the function expand_as()\n","        p_h_given_v = torch.sigmoid(activation)\n","#using a bernoulli RBM because we are predicting a binary outcome\n","        return p_h_given_v, torch.bernoulli(p_h_given_v)\n","    \n","#sampling the visible nodes given the hidden nodes\n","    def sample_v(self,y):\n","        wy = torch.mm(y, self.W) #no transpose\n","        activation = wy + self.b.expand_as(wy)\n","        p_v_given_h = torch.sigmoid(activation)\n","        return p_v_given_h, torch.bernoulli(p_v_given_h)\n","\n","#creating CONTRASTIVE DIVERGENCE in train\n","#CD is about approximating the log-likelihood gradient. RBM is an energy-based model and can\n","#be seen as a Probability Graphical Model.\n","#GOAL is to MINIMISE the energy (Energy_Based_Model) or MAXIMISE the the Log-Likelihood \n","#function (Prob Graphical Model). In both cases we compute the Gradient. CD comes with Gibbs Sampling\n","#GS consists of creating a Gibbs Chain in k-steps by sampling k-times the hidden nodes and visible nodes\n","#Algorithm in Page 25 of the article(An introduction to RBMs)[codes 8-10]\n","    def train(self, v0, vk, ph0, phk):\n","#v0=input vector containing the ratings of all movies by one user (loop for all users)\n","#vk=visible nodes obtained after k-sampling[visible-hidden-visible (round trip)]\n","#ph0=vector of prob that at the first iteration, the hidden nodes equal one(1) given the values of v0\n","#phk=prob of hidden nodes after k-sampling given the values of the visible nodes\n","#line 8\n","        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t() #hidden so transpose\n","#line 9\n","        self.b += torch.sum((v0-vk), 0) #0 in order to keep the dimension\n","#line 10\n","        self.a += torch.sum((ph0-phk), 0)\n","#we can add more parameters like learning rate to improve and tune the model\n","\n","#creating an object of our RBM class\n","nv = len(training_set[0])\n","nh = 100 #tunable\n","batch_size = 100 #tunable (total train data = 943)\n","rbm = RBM(nv, nh)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gy59alAdloL"},"source":["## Training the RBM\n"]},{"cell_type":"code","metadata":{"id":"FEz9hRaciFTs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630161087965,"user_tz":-300,"elapsed":18789,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"b29fa1f2-da9c-4546-f12a-d4121f6aa145"},"source":["nb_epochs = 10 #tunable\n","for epoch in range(1, nb_epochs +1):\n","    train_loss = 0 #using Simple Difference in Abs Value (others RMSE)\n","    s = 0. #need a counter to normalise the train loss\n","    for id_user in range(0, nb_users-batch_size, batch_size): \n","        vk = training_set[id_user:id_user+batch_size] #here, vk is the input at the start \n","#vk=input vector going into the Gibbs chain whcih will be updated at each round trip\n","        v0 = training_set[id_user:id_user+batch_size]\n","#v0=targets(real), ratings given already\n","        ph0,_ = rbm.sample_h(v0) #v0=visible nodes\n","#ph0=initial prob, from the sample_h, we return only the first result\n","#for loop for k steps of CD for Gibbs Chain[MCMC technique]\n","        for k in range(10):\n","            _,hk = rbm.sample_h(vk) #hk=hidden nodes at kth step of CD\n","            _,vk = rbm.sample_v(hk) #first update of the visible node after 1st sampling\n","            vk[v0<0] = v0[v0<0] #exclude cells with no ratings [-1]\n","#compute for phk before applying train function\n","        phk,_ = rbm.sample_h(vk) #last sample of visible nodes after 10th step\n","        rbm.train(v0, vk, ph0, phk)\n","        train_loss += torch.mean(torch.abs(v0[v0>=0]-vk[v0>=0]))\n","# USING RMSE for training\n","        #train_loss += np.sqrt(torch.mean((v0[v0>=0]-vk[v0>0])**2))\n","        s += 1.\n","    print('epoch: ' + str(epoch) + ' loss: '+ str(train_loss/s))\n","\n","nb_epoch = 10\n","for epoch in range(1, nb_epoch + 1):\n","  train_loss = 0\n","  s = 0.\n","  for id_user in range(0, nb_users - batch_size, batch_size):\n","    vk = training_set[id_user : id_user + batch_size]\n","    v0 = training_set[id_user : id_user + batch_size]\n","    ph0,_ = rbm.sample_h(v0)\n","    for k in range(10):\n","      _,hk = rbm.sample_h(vk)\n","      _,vk = rbm.sample_v(hk)\n","      vk[v0<0] = v0[v0<0]\n","    phk,_ = rbm.sample_h(vk)\n","    rbm.train(v0, vk, ph0, phk)\n","    train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n","    s += 1.\n","  print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["epoch: 1 loss: tensor(0.2082)\n","epoch: 2 loss: tensor(0.0103)\n","epoch: 3 loss: tensor(0.0032)\n","epoch: 4 loss: tensor(0.0020)\n","epoch: 5 loss: tensor(0.0013)\n","epoch: 6 loss: tensor(0.0010)\n","epoch: 7 loss: tensor(0.0009)\n","epoch: 8 loss: tensor(0.0008)\n","epoch: 9 loss: tensor(0.0007)\n","epoch: 10 loss: tensor(0.0006)\n","epoch: 1 loss: tensor(0.0007)\n","epoch: 2 loss: tensor(0.0005)\n","epoch: 3 loss: tensor(0.0004)\n","epoch: 4 loss: tensor(0.0006)\n","epoch: 5 loss: tensor(0.0004)\n","epoch: 6 loss: tensor(0.0005)\n","epoch: 7 loss: tensor(0.0003)\n","epoch: 8 loss: tensor(0.0005)\n","epoch: 9 loss: tensor(0.0005)\n","epoch: 10 loss: tensor(0.0005)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bak5uc8gd-gX"},"source":["## Testing the RBM\n"]},{"cell_type":"code","metadata":{"id":"5ztvzYRtiGCz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630161087966,"user_tz":-300,"elapsed":6,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"55077074-a0cd-4574-9904-a734c114abbe"},"source":["#testing the rbm on test data\n","test_loss = 0\n","s = 0.\n","for id_user in range(nb_users):\n","    v = training_set[id_user:id_user+1] #inputs\n","    vt = test_set[id_user:id_user+1] #targets\n","    if len(vt[vt>=0])>0:\n","        _,h = rbm.sample_h(v)\n","        _,v = rbm.sample_v(h)\n","        test_loss += torch.mean(torch.abs(vt[vt>=0]-v[vt>=0]))  \n","# USING RMSE for testing\n","        #test_loss += np.sqrt(torch.mean((vt[vt>=0]-v[vt>0])**2))\n","        s +=1\n","print('test loss: ' +str(test_loss/s))\n","\n","# Test Loss Value should be approximately equal to the Train Loss Value"],"execution_count":12,"outputs":[{"output_type":"stream","text":["test loss: tensor(0.0004)\n"],"name":"stdout"}]}]}